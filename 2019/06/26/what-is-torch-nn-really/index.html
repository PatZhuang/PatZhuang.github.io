<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">








  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">














  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext">
  






<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">

<link rel="stylesheet" href="/css/main.css?v=7.1.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon/apple-touch-icon.png?v=7.1.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon/favicon.ico?v=7.1.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon/favicon-32x32.png?v=7.1.2">








<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="æœ¬æ–‡ä¸º WHAT IS TORCH.NN REALLY? çš„å­¦ä¹ ç¬”è®°">
<meta name="keywords" content="pytorch">
<meta property="og:type" content="article">
<meta property="og:title" content="What Is torch.nn Really?">
<meta property="og:url" content="http://yoursite.com/2019/06/26/what-is-torch-nn-really/index.html">
<meta property="og:site_name" content="Pato&#39;s raison d&#39;etre">
<meta property="og:description" content="æœ¬æ–‡ä¸º WHAT IS TORCH.NN REALLY? çš„å­¦ä¹ ç¬”è®°">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://pytorch.org/tutorials/_images/sphx_glr_nn_tutorial_001.png">
<meta property="og:updated_time" content="2019-06-28T02:32:02.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="What Is torch.nn Really?">
<meta name="twitter:description" content="æœ¬æ–‡ä¸º WHAT IS TORCH.NN REALLY? çš„å­¦ä¹ ç¬”è®°">
<meta name="twitter:image" content="https://pytorch.org/tutorials/_images/sphx_glr_nn_tutorial_001.png">





  
  
  <link rel="canonical" href="http://yoursite.com/2019/06/26/what-is-torch-nn-really/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>What Is torch.nn Really? | Pato's raison d'etre</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Pato's raison d'etre</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="åˆ‡æ¢å¯¼èˆªæ ">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>é¦–é¡µ</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>æ ‡ç­¾</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>å…³äº</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>å½’æ¡£</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/06/26/what-is-torch-nn-really/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Patrick">
      <meta itemprop="description" content="ğŸŒˆ">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Pato's raison d'etre">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">What Is torch.nn Really?

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">å‘è¡¨äº</span>
              

              
                
              

              <time title="åˆ›å»ºæ—¶é—´ï¼š2019-06-26 20:22:26" itemprop="dateCreated datePublished" datetime="2019-06-26T20:22:26+08:00">2019-06-26</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">æ›´æ–°äº</span>
                
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2019-06-28 10:32:02" itemprop="dateModified" datetime="2019-06-28T10:32:02+08:00">2019-06-28</time>
              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>æœ¬æ–‡ä¸º <a href="https://pytorch.org/tutorials/beginner/nn_tutorial.html" target="_blank" rel="noopener">WHAT IS TORCH.NN REALLY?</a> çš„å­¦ä¹ ç¬”è®°</p>
<a id="more"></a>

<p>PyTorch æä¾›äº†è®¾è®¡ä¼˜é›…çš„æ¨¡å—å’Œç±»ä¾‹å¦‚ torch.nn, torch.optim, Dataset å’Œ Dataloader æ¥å¸®åŠ©ä½ åˆ›å»ºå’Œè®­ç»ƒç¥ç»ç½‘ç»œã€‚ä¸ºäº†å……åˆ†è¿ç”¨è¿™äº›æ¨¡å—ï¼Œä½ éœ€è¦çœŸæ­£ç†è§£å®ƒä»¬åœ¨åšäº›ä»€ä¹ˆã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨ MNIST æ•°æ®é›†ä½¿ç”¨åŸºæœ¬çš„ PyTorch tensor æ¥è®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œè€Œä¸ä½¿ç”¨ä¸Šè¿°çš„æ¨¡å—ã€‚ä¹‹åæˆ‘ä»¬å°†é€æ­¥æ·»åŠ  <code>torch.nn</code>, <code>torch.optim</code>, <code>Dataset</code> å’Œ <code>DataLoader</code> çš„åŠŸèƒ½æ¥å±•ç¤ºè¿™äº›æ¨¡å—åˆ†åˆ«åšäº†ä»€ä¹ˆï¼Œä»¥åŠå®ƒä»¬æ˜¯å¦‚ä½•ä½¿å¾—ä»£ç æ›´åŠ ç®€æ´å’Œçµæ´»ã€‚</p>
<h2 id="MNIST-data-setup"><a href="#MNIST-data-setup" class="headerlink" title="MNIST data setup"></a>MNIST data setup</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path <span class="comment"># ä½¿ç”¨ pathlib æ¥å¤„ç†è·¯å¾„</span></span><br><span class="line"><span class="keyword">import</span> requests <span class="comment"># ä½¿ç”¨ requests æ¥ä¸‹è½½ MNIST æ•°æ®é›†</span></span><br><span class="line"></span><br><span class="line">DATA_PATH = Path(<span class="string">'data'</span>)</span><br><span class="line">PATH = DATA_PATH / <span class="string">'mnist'</span></span><br><span class="line"></span><br><span class="line">PATH.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>) </span><br><span class="line"></span><br><span class="line">URL = <span class="string">'http://deeplearning.net/data/mnist/'</span></span><br><span class="line">FILENAME = <span class="string">'mnist.pkl.gz'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> (PATH / FILENAME).exists():</span><br><span class="line">    content = requests.get(URL + FILENAME).content</span><br><span class="line">    (PATH / FILENAME).open(<span class="string">'wb'</span>).write(content)</span><br></pre></td></tr></table></figure>

<p>æ•°æ®é›†æ˜¯ numpy array æ ¼å¼ï¼Œä¸”è¢« pickle åºåˆ—åŒ–ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"></span><br><span class="line"><span class="comment"># as_posix() ä» PosixPath ä¸­å–å‡º Unix è·¯å¾„åˆ†éš”ç¬¦ï¼ˆ / ï¼‰è¡¨ç¤ºçš„è·¯å¾„</span></span><br><span class="line"><span class="keyword">with</span> gzip.open((PATH / FILENAME).as_posix(), <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=<span class="string">'latin-1'</span>)</span><br></pre></td></tr></table></figure>

<p>æ¯å¼ å›¾ç‰‡æ˜¯ 28x28 çš„å¤§å°ï¼Œè¢«å±•å¹³ä¸ºé•¿åº¦ä¸º 784ï¼ˆ28x28ï¼‰çš„è¡Œã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">plt.imshow(x_train[<span class="number">0</span>].reshape((<span class="number">28</span>, <span class="number">28</span>)), cmap=<span class="string">'gray'</span>)</span><br><span class="line">print(x_train.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(50000, 784)</code></pre><p><img src="https://pytorch.org/tutorials/_images/sphx_glr_nn_tutorial_001.png" alt="svg"></p>
<p>PyTorch ä½¿ç”¨ <code>torch.tensor</code> è€Œä¸æ˜¯ numpy arrayï¼Œæ‰€ä»¥è¦è¿›è¡Œæ•°æ®æ ¼å¼è½¬æ¢ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># æŠŠ numpy array è½¬æ¢ä¸º torch tensor</span></span><br><span class="line"><span class="comment"># map å¯¹è±¡å¯ä»¥ç›´æ¥æŒ‰ä½ç½®è§£åŒ…</span></span><br><span class="line">x_train, y_train, x_valid, y_valid = map(</span><br><span class="line">    torch.tensor, (x_train, y_train, x_valid, y_valid)</span><br><span class="line">)</span><br><span class="line">n, c = x_train.shape</span><br><span class="line">x_train, x_train.shape, y_train.min(), y_train.max()</span><br><span class="line">print(x_train, y_train)</span><br><span class="line">print(x_train.shape)</span><br><span class="line">print(y_train.min(), y_train.max())</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])
torch.Size([50000, 784])
tensor(0) tensor(9)</code></pre><h2 id="Neural-net-from-scratch-no-torch-nn"><a href="#Neural-net-from-scratch-no-torch-nn" class="headerlink" title="Neural net from scratch(no torch.nn)"></a>Neural net from scratch(no torch.nn)</h2><p>é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨ PyTorch tensor çš„è¿ç®—æ¥æ„å»ºä¸€ä¸ªæ¨¡å‹ã€‚</p>
<p>PyTorch æä¾›äº†å°† Tensor éšæœºåˆå§‹åŒ–æˆ–åˆå§‹åŒ–ä¸ºå…¨ 0 çš„æ–¹æ³•ï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™äº›æ–¹æ³•æ¥åˆ›å»ºæƒé‡å’Œ bias. è¿™ä¸¤è€…éƒ½æ˜¯ä¸€èˆ¬çš„ tensorï¼Œå”¯ä¸€ä¸åŒä¹‹å¤„æ˜¯ï¼Œæˆ‘ä»¬ä¼šå°†å®ƒä»¬çš„ <code>requires_grad</code> è®¾ç½®ä¸º <code>True</code>. è¿™ä½¿å¾— PyTorch ä¼šè®°å½•æ‰€æœ‰åœ¨è¿™äº› tensor ä¸Šæ‰§è¡Œçš„è¿ç®—ï¼Œåœ¨åå‘ä¼ æ’­æœŸé—´å°±å¯ä»¥è‡ªåŠ¨è®¡ç®—ç›¸åº”çš„æ¢¯åº¦ã€‚</p>
<blockquote>
<p>æˆ‘ä»¬ä½¿ç”¨ <a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" target="_blank" rel="noopener">Xavier initialization</a> æ¥åˆå§‹åŒ–æƒé‡ï¼Œä¹Ÿå³å°†æ¯ä¸ªå€¼éƒ½é™¤ä»¥ sqrt(n).</p>
<p>è¿™ä¸€æ­¥æ‰§è¡Œä¹‹åå†è®¾ç½® requires_grad ä¸º Trueï¼Œå› ä¸ºæˆ‘ä»¬ä¸éœ€è¦è®¡ç®—è¿™ä¸ªé™¤æ³•è¿ç®—å¯¹åº”çš„æ¢¯åº¦ã€‚</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">weights = torch.randn(<span class="number">784</span>, <span class="number">10</span>) / math.sqrt(<span class="number">784</span>)</span><br><span class="line">weights.requires_grad_()</span><br><span class="line">bias = torch.zeros(<span class="number">10</span>, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>ç”±äº PyTorch çš„è‡ªåŠ¨æ¢¯åº¦è¿ç®—æœºåˆ¶ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»»æ„çš„æ ‡å‡† Python å‡½æ•°ï¼ˆæˆ– callale å¯¹è±¡ï¼‰ä½œä¸ºæ¨¡å‹ã€‚</p>
<p>æ¥ä¸‹æ¥ä½¿ç”¨ç®€å•çš„çŸ©é˜µä¹˜æ³•å’ŒåŠ æ³•å¹¿æ’­æ¥æ„å»ºä¸€ä¸ªçº¿æ€§æ¨¡å‹ï¼Œæ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜éœ€è¦ä½¿ç”¨ä¸€ä¸ªæ¿€åŠ±å‡½æ•°ï¼Œè¿™é‡Œä½¿ç”¨æ‰‹å†™çš„ <em>log_soft_max</em> å‡½æ•°ã€‚</p>
<p>æ³¨æ„ï¼šè™½ç„¶ PyTorch æä¾›äº†è®¸å¤šå¸¸ç”¨çš„ loss å’Œæ¿€åŠ±å‡½æ•°ï¼Œä½†ä½ ä»ç„¶å¯ä»¥æ‰‹åŠ¨å®ç°ä½ æƒ³è¦çš„ loss å’Œ activation func, è€Œä¸” PyTorch ä»ç„¶ä¼šä¸ºè¿™äº›æ‰‹åŠ¨å®ç°çš„è¿ç®—æä¾› GPU æˆ– å‘é‡åŒ– CPU åŠ é€Ÿã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_softmax</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment"># æ‰§è¡Œå®Œ x.exp().sum(-1) å shape ä¼šå˜æˆ (64)</span></span><br><span class="line">    <span class="comment"># ä¸ºäº†èƒ½å¤Ÿå¹¿æ’­è¦åœ¨æœ€ååŠ ä¸Šä¸€ç»´ï¼Œæ‰€ä»¥è°ƒç”¨ unsqueeze(-1), å˜æˆ (64, 1)</span></span><br><span class="line">    <span class="keyword">return</span> x - x.exp().sum(<span class="number">-1</span>).log().unsqueeze(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(xb)</span>:</span></span><br><span class="line">    <span class="comment"># xb @ weights + bias çš„ shape æ˜¯ (64, 10)</span></span><br><span class="line">    <span class="comment"># @ æ˜¯çŸ©é˜µä¹˜æ³•ï¼Œï¼ˆå¤šæ•°æƒ…å†µä¸‹ï¼‰ç›¸å½“äº torch.mm()</span></span><br><span class="line">    <span class="keyword">return</span> log_softmax(xb @ weights + bias)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bs = <span class="number">64</span> <span class="comment"># batch size</span></span><br><span class="line"></span><br><span class="line">xb = x_train[<span class="number">0</span>:bs] <span class="comment"># (64, 784)</span></span><br><span class="line">preds = model(xb)</span><br><span class="line">print(preds[<span class="number">0</span>], preds.shape)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([-2.6260, -1.9875, -2.5530, -2.1999, -1.8609, -2.5637, -2.6143, -2.1218,
        -2.6210, -2.2671], grad_fn=&lt;SelectBackward&gt;) torch.Size([64, 10])</code></pre><p>å¯ä»¥çœ‹åˆ°ï¼Œ<code>preds</code> å¼ é‡ä¸ä»…åŒ…å« tensor å€¼ï¼Œä¹ŸåŒ…å«äº†æ¢¯åº¦å‡½æ•°ï¼Œä¹‹åæˆ‘ä»¬ä¼šåœ¨åå‘ä¼ æ’­çš„æ—¶å€™ä½¿ç”¨å®ƒã€‚</p>
<p>æ¥ä¸‹æ¥å®ç°ä¸€ä¸ªè´Ÿçš„ log-likelihood ï¼ˆä¹Ÿå°±æ˜¯ cross entropyï¼‰ä½œä¸º loss func.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nll</span><span class="params">(input, target)</span>:</span></span><br><span class="line">    <span class="comment"># å¯¹äº log softmax çš„ cross entropy</span></span><br><span class="line">    <span class="comment"># è®¡ç®—å…¬å¼æ˜¯ E(y, t) = -y[t], å…¶ä¸­ t æ˜¯ target å¯¹åº”çš„ index</span></span><br><span class="line">    <span class="keyword">return</span> -input[range(target.shape[<span class="number">0</span>]), target].mean()</span><br><span class="line"></span><br><span class="line">loss_func = nll</span><br></pre></td></tr></table></figure>

<p>æ¥çœ‹çœ‹éšæœºåˆå§‹åŒ–æƒé‡çš„æ¨¡å‹çš„æŸå¤±ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yb = y_train[<span class="number">0</span>:bs]</span><br><span class="line">print(loss_func(preds, yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(2.3394, grad_fn=&lt;NegBackward&gt;)</code></pre><p>æ¥ä¸‹æ¥å†å®ç°ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—æ¨¡å‹çš„æ­£ç¡®ç‡ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(out, yb)</span>:</span></span><br><span class="line">    preds = torch.argmax(out, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (preds == yb).float().mean().item()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(accuracy(preds, yb))</span><br></pre></td></tr></table></figure>

<pre><code>0.171875</code></pre><p>æ¥ä¸‹æ¥å¯ä»¥æ‰§è¡Œè®­ç»ƒäº†ã€‚åœ¨æ¯è½®è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬å°†ï¼š</p>
<ul>
<li>é€‰æ‹©ä¸€ä¸ª mini-batchï¼ˆsize å¤§å°ä¸º <code>bs</code>ï¼‰</li>
<li>ä½¿ç”¨æ¨¡å‹æ¥è¿›è¡Œé¢„æµ‹</li>
<li>è®¡ç®— loss</li>
<li><code>loss.backward()</code> æ¥æ›´æ–° <code>weights</code> å’Œ <code>bias</code></li>
</ul>
<p>åœ¨æ›´æ–° weights å’Œ bias çš„æ—¶å€™ï¼Œæˆ‘ä»¬åœ¨ <code>torch.no_grad()</code> ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œï¼Œè¿™æ˜¯å› ä¸ºæˆ‘ä»¬ä¸éœ€è¦å¯¹æƒé‡æ›´æ–°æ“ä½œæ±‚æ¢¯åº¦ã€‚</p>
<p>åœ¨æ¯è½®è¿­ä»£ç»“æŸåè¦å°†æ¨¡å‹çš„æ¢¯åº¦é‡ç½®ä¸º 0ï¼Œå¦åˆ™æ¯è½®è®¡ç®—çš„æ¢¯åº¦ä¼šè¢«ç´¯åŠ ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> IPython.core.debugger <span class="keyword">import</span> set_trace</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.5</span> <span class="comment"># learning_rate</span></span><br><span class="line">epochs = <span class="number">2</span> <span class="comment"># how many epochs to train for</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line"><span class="comment">#         set_trace()</span></span><br><span class="line">        start_i = i * bs</span><br><span class="line">        end_i = start_i + bs</span><br><span class="line">        xb = x_train[start_i:end_i]</span><br><span class="line">        yb = y_train[start_i:end_i]</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line">        </span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            weights -= weights.grad * lr</span><br><span class="line">            bias -= bias.grad * lr</span><br><span class="line">            weights.grad.zero_()</span><br><span class="line">            bias.grad.zero_()</span><br></pre></td></tr></table></figure>

<p>åˆ°è¿™é‡Œä¸ºæ­¢ï¼Œæˆ‘ä»¬æˆåŠŸä»å¤´åˆ›å»ºå¹¶è®­ç»ƒäº†ä¸€ä¸ªæå°çš„ç¥ç»ç½‘ç»œï¼ˆä¸€ä¸ªé€»è¾‘å›å½’ï¼‰ã€‚</p>
<p>å†æ¬¡æ£€æŸ¥æ­£ç¡®ç‡å’Œ lossï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(loss_func(model(xb), yb), accuracy(model(xb), yb), sep=<span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>tensor(0.0798, grad_fn=&lt;NegBackward&gt;)
1.0</code></pre><h2 id="Using-torch-nn-functional"><a href="#Using-torch-nn-functional" class="headerlink" title="Using torch.nn.functional"></a>Using torch.nn.functional</h2><p>æ¥ä¸‹æ¥æˆ‘ä»¬è¦é€æ­¥é‡æ„ä»£ç æ¥ä½¿å…¶æ›´åŠ ç®€æ´ã€æ˜“æ‡‚å’Œçµæ´»ã€‚</p>
<p>ç¬¬ä¸€æ­¥ä¹Ÿæ˜¯æœ€ç®€å•çš„ä¸€æ­¥ï¼Œä½¿ç”¨ <code>torch.nn.functional</code> ï¼ˆä¸€èˆ¬ alias ä¸º <code>F</code>ï¼‰ä¸­çš„å¯¹åº”å‡½æ•°æ¥æ›¿ä»£æˆ‘ä»¬æ‰‹å†™çš„æ¿€åŠ±å‡½æ•°å’ŒæŸå¤±å‡½æ•°ã€‚è¿™ä¸ªæ¨¡å—åŒ…å«äº† <code>torch.nn</code> ä¸­çš„æ‰€æœ‰å‡½æ•°ï¼ˆä¸åŒ…æ‹¬å…¶ä¸­çš„ç±»ï¼‰ã€‚é™¤äº†æ¿€åŠ±å‡½æ•°å’ŒæŸå¤±å‡½æ•°ä»¥å¤–ï¼Œè¯¥æ¨¡å—è¿˜åŒ…å«äº†ä¸€äº›ç”¨äºæ„å»ºç¥ç»ç½‘ç»œçš„å‡½æ•°ï¼Œä¾‹å¦‚æ± åŒ–å‡½æ•°å’Œç”¨äºæ„å»ºå·ç§¯å±‚å’Œçº¿æ€§å±‚çš„å‡½æ•°ç­‰ï¼Œä¸è¿‡è¿™äº›å‡½æ•°ä½¿ç”¨å¯¹åº”çš„ç±»æ¥æ›¿æ¢æ›´å¥½ã€‚</p>
<p>å¦‚æœæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ negtive log likelihood ä½œä¸º loss å’Œ softmax ä½œä¸ºæ¿€åŠ±å‡½æ•°ï¼ŒPyTorch ä¸­å¯ä»¥ä½¿ç”¨ <code>F.cross_entropy</code> æ¥æ•´åˆä¸Šè¿°ä¸¤è€…ï¼Œå› æ­¤æˆ‘ä»¬ç”šè‡³å¯ä»¥åœ¨æ¨¡å‹ä¸­ç§»é™¤æ¿€åŠ±å‡½æ•°ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">loss_func = F.cross_entropy</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(xb)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> xb @ weights + bias</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(loss_func(model(xb), yb), accuracy(model(xb), yb), sep=<span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>tensor(0.0798, grad_fn=&lt;NllLossBackward&gt;)
1.0</code></pre><p>å¯ä»¥çœ‹åˆ° loss å’Œ acc å’ŒåŸæ¥æ˜¯ä¸€æ ·çš„ã€‚</p>
<h2 id="Refactor-using-nn-Module"><a href="#Refactor-using-nn-Module" class="headerlink" title="Refactor using nn.Module"></a>Refactor using nn.Module</h2><p>æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨ <code>nn.Module</code> å’Œ <code>nn.Parameter</code> æ¥é‡æ„è®­ç»ƒå¾ªç¯ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ª <code>nn.Module</code> çš„å­ç±»æ¥ä¿å­˜æƒé‡å’Œ bias ä»¥åŠå‰å‘ä¼ æ’­çš„æ–¹æ³•ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mnist_Logistic</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.weights = nn.Parameter(torch.randn(<span class="number">784</span>, <span class="number">10</span>) / math.sqrt(<span class="number">784</span>))</span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(<span class="number">10</span>))</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, xb)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> xb @ self.weights + self.bias</span><br></pre></td></tr></table></figure>

<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># åˆå§‹åŒ–æ¨¡å‹</span></span><br><span class="line"><span class="attr">model</span> = Mnist_Logistic()</span><br></pre></td></tr></table></figure>

<p>æ¥ä¸‹æ¥æˆ‘ä»¬åƒä¹‹å‰ä¸€æ ·è®¡ç®— loss. æ³¨æ„åˆ°è¿™é‡Œçš„ model è¢«å½“æˆ function æ¥è°ƒç”¨ï¼Œä½†æ˜¯å®é™…ä¸Š PyTorch æ˜¯è°ƒç”¨äº† model çš„ <code>forward</code> æ–¹æ³•ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(2.3865, grad_fn=&lt;NllLossBackward&gt;)</code></pre><p>ç°åœ¨æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ model.parameters() å’Œ model.zero_grad() æ¥ç®€åŒ–æƒé‡æ›´æ–°çš„æ“ä½œã€‚</p>
<pre>
# before:
with torch.no_grad():
    weights -= weights.grad * lr
    bias -= bias.grad * lr
    weights.grad.zero_()
    bias.grad.zero_()

# after:
with torch.no_grad():
    for p in model.parameters():
        p -= p.grad * lr
    model.zero_grad()
</pre>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line">            start_i = i * bs</span><br><span class="line">            end_i = start_i + bs</span><br><span class="line">            xb = x_train[start_i: end_i]</span><br><span class="line">            yb = y_train[start_i: end_i]</span><br><span class="line">            pred = model(xb)</span><br><span class="line">            loss = loss_func(pred, yb)</span><br><span class="line">            </span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">                    p -= p.grad * lr</span><br><span class="line">                model.zero_grad()</span><br><span class="line"></span><br><span class="line">fit()</span><br></pre></td></tr></table></figure>

<p>å†æ¬¡ç¡®è®¤ loss æ˜¯å¦ä¸‹é™äº†ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(0.0816, grad_fn=&lt;NllLossBackward&gt;)</code></pre><h2 id="Refactor-using-nn-Linear"><a href="#Refactor-using-nn-Linear" class="headerlink" title="Refactor using nn.Linear"></a>Refactor using nn.Linear</h2><p>æ¥ä¸‹æ¥ä½¿ç”¨ nn.Linear æ¥æ›¿æ¢ <code>xb @ self.weights + self.bias</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mnist_Logistic</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.lin = nn.Linear(<span class="number">784</span>, <span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, xb)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.lin(xb)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = Mnist_Logistic()</span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(2.2176, grad_fn=&lt;NllLossBackward&gt;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># é‡å¤è®­ç»ƒæ­¥éª¤</span></span><br><span class="line">fit()</span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(0.0807, grad_fn=&lt;NllLossBackward&gt;)</code></pre><h2 id="Refactor-using-optim"><a href="#Refactor-using-optim" class="headerlink" title="Refactor using optim"></a>Refactor using optim</h2><p>PyTorch çš„ <code>torch.optim</code> åŒ…ä¹Ÿæä¾›äº†å¤šç§ä¼˜åŒ–ç®—æ³•ã€‚å¯ä»¥ä½¿ç”¨å…¶å†…ç½®ä¼˜åŒ–å™¨çš„ <code>step</code> æ–¹æ³•æ¥æ›¿ä»£æ‰‹åŠ¨æƒé‡æ›´æ–°ã€‚</p>
<pre>
# before:
with torch.no_grad():
    for p in model.parameters():
        p -= p.grad * lr
    model.zero_grad()

# after:
# ä¸éœ€è¦å†ä½¿ç”¨ torch.no_grad() ä¸Šä¸‹æ–‡
opt.step()
opt.zero_grad() # æ¸…ç©ºæ¢¯åº¦ç°åœ¨ç”±ä¼˜åŒ–å™¨æ¥æ‰§è¡Œ
</pre>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ„é€ ä¸€ä¸ªè¾…åŠ©å‡½æ•°æ¥è¿”å›æˆ‘ä»¬éœ€è¦çš„æ¨¡å‹å’Œä¼˜åŒ–å™¨</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_model</span><span class="params">()</span>:</span></span><br><span class="line">    model = Mnist_Logistic()</span><br><span class="line">    <span class="keyword">return</span> model, optim.SGD(model.parameters(), lr = lr)</span><br><span class="line"></span><br><span class="line">model, opt = get_model()</span><br><span class="line">print(loss_func(model(xb), yb))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line">        start_i = i * bs</span><br><span class="line">        end_i = start_i + bs</span><br><span class="line">        xb = x_train[start_i:end_i]</span><br><span class="line">        yb = y_train[start_i:end_i]</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line">        </span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(2.3893, grad_fn=&lt;NllLossBackward&gt;)
tensor(0.0819, grad_fn=&lt;NllLossBackward&gt;)</code></pre><h2 id="Refactor-using-Dataset"><a href="#Refactor-using-Dataset" class="headerlink" title="Refactor using Dataset"></a>Refactor using Dataset</h2><p>PyTorch æœ‰ä¸€ä¸ªæŠ½è±¡çš„ Dataset ç±»ã€‚ä»»ä½•ç±»éƒ½å¯ä»¥æ˜¯ä¸€ä¸ª Datasetï¼Œåªè¦å®ƒæœ‰ <code>__len__</code> å’Œ <code>__getitem__</code> æ–¹æ³•ã€‚æ›´å¤š Dataset ç±»çš„ä½¿ç”¨æ–¹æ³•å¯ä»¥å‚ç…§ <a href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html" target="_blank" rel="noopener">DATA LOADING AND PROCESSING TUTORIAL</a>.</p>
<p>PyTorch çš„ TensorDataset æ˜¯ä¸€ä¸ªå°è£…äº† tensor çš„ Dataset. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># x_train å’Œ y_train å¯ä»¥ç”¨ TensorDataset å°è£…åˆ°ä¸€èµ·</span></span><br><span class="line"><span class="comment"># è¿™æ ·å¯ä»¥å¤§å¤§æ–¹ä¾¿è¿­ä»£</span></span><br><span class="line">train_ds = TensorDataset(x_train, y_train)</span><br></pre></td></tr></table></figure>

<pre>
# before:
xb = x_train[start_i : end_i]
yb = y_train[start_i : end_i]

# after:
xb, yb = train_ds[i*bs : i*bs+bs]
</pre>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">model, opt = get_model()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line">        xb, yb = train_ds[i * bs : i * bs + bs]</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line">        </span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(0.0805, grad_fn=&lt;NllLossBackward&gt;)</code></pre><h2 id="Refactor-using-DataLoader"><a href="#Refactor-using-DataLoader" class="headerlink" title="Refactor using DataLoader"></a>Refactor using DataLoader</h2><p><code>DataLoader</code> ç”¨äºç®¡ç† batches. ä½ å¯ä»¥ä½¿ç”¨ä»»æ„çš„ <code>Dataset</code> æ„é€  <code>DataLoader</code>. <code>DataLoader</code> ä½¿å¾— batch çš„è¿­ä»£æ›´åŠ æ–¹ä¾¿ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_ds = TensorDataset(x_train, y_train)</span><br><span class="line">train_dl = DataLoader(train_ds, batch_size=<span class="number">64</span>)</span><br></pre></td></tr></table></figure>

<pre>
# before:
for i in range((n-1)//bs+1):
    xb, yb = train_ds[i*bs : i*bs+1]
    pred = model(xb)

# after:
for xb, yb in train_dl:
    pred = model(xb)
</pre>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model, opt = get_model()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    <span class="keyword">for</span> xb, yb <span class="keyword">in</span> train_dl:</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line">        </span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line">        </span><br><span class="line">print(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>

<pre><code>tensor(0.0809, grad_fn=&lt;NllLossBackward&gt;)</code></pre><p>ä½¿ç”¨ <code>nn.Module</code>, <code>nn.Parameter</code>, <code>Dataset</code> å’Œ <code>DataLoader</code> é‡æ„åï¼Œè®­ç»ƒå¾ªç¯ç°åœ¨å˜å¾—éå¸¸ç®€æ´æ˜“æ‡‚ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬å†åŠ å…¥ä¸€äº›æ¨¡å‹è®­ç»ƒæ—¶çš„å¸¸ç”¨æ“ä½œã€‚</p>
<h2 id="Add-Validation"><a href="#Add-Validation" class="headerlink" title="Add Validation"></a>Add Validation</h2><p>åœ¨å®é™…çš„æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸ºäº†åˆ¤æ–­æ¨¡å‹æ˜¯å¦è¿‡æ‹Ÿåˆï¼Œéœ€è¦é…åˆä½¿ç”¨éªŒè¯é›†ï¼ˆvalidation setï¼‰ã€‚</p>
<p>æ´—æ··ï¼ˆshufflingï¼‰è®­ç»ƒæ•°æ®ï¼ˆtraining dataï¼‰ä¹Ÿæ˜¯é˜²æ­¢ batch é¡ºåºå¯¼è‡´è¿‡æ‹Ÿåˆçš„é‡è¦æ­¥éª¤ï¼Œè¯¦è§£è§ï¼š<a href="https://www.quora.com/Does-the-order-of-training-data-matter-when-training-neural-networks" target="_blank" rel="noopener">Does the order of training data matter when training neural networks?</a>ã€‚ä½†æ˜¯å¯¹äºéªŒè¯é›†æ¥è¯´ï¼Œæ˜¯å¦æ´—æ··å¯¹ loss æ²¡æœ‰å½±å“ï¼Œå› æ­¤ä¸€èˆ¬ä¸æ´—æ··éªŒè¯é›†ã€‚</p>
<p>åœ¨éªŒè¯é›†ä¸Šï¼Œæˆ‘ä»¬ä½¿ç”¨çš„ batch size æ˜¯ä¸¤å€äºè®­ç»ƒé›†çš„ batch sizeï¼Œè¿™æ˜¯å› ä¸ºéªŒè¯é›†ä¸Šä¸éœ€è¦æ‰§è¡Œåå‘ä¼ æ’­ï¼Œå› æ­¤ä¼šå ç”¨æ›´å°çš„å†…å­˜ï¼ˆå› ä¸ºä¸å­˜å‚¨æ¢¯åº¦ï¼‰ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ›´å¤§çš„ batch æ¥åŠ å¿« loss çš„è®¡ç®—ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train_ds = TensorDataset(x_train, y_train)</span><br><span class="line">train_dl = DataLoader(train_ds, batch_size=bs, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">valid_ds = TensorDataset(x_valid, y_valid)</span><br><span class="line">valid_dl = DataLoader(valid_ds, batch_size=bs * <span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p>æ¥ä¸‹æ¥åœ¨æ¯ä¸ª epoch è¿­ä»£ç»“æŸçš„æ—¶å€™è®¡ç®— validation loss.</p>
<p>æ³¨æ„åœ¨è®­ç»ƒæ¨¡å‹å‰è¦è°ƒç”¨ <code>model.train()</code>ï¼Œåœ¨è¿›è¡Œæ¨æ–­ä¹‹å‰è¦è°ƒç”¨ <code>model.eval()</code>ã€‚è¿™ä¸¤ä¸ªæ–¹æ³•æ˜¯å†³å®š <code>nn.BatchNorm2d</code> å’Œ <code>nn.Dropout</code> è¿™ä¸€ç±»çš„å±‚åœ¨è®­ç»ƒå’Œæ¨æ–­é˜¶æ®µçš„å¼€å¯å’Œå…³é—­ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">model, opt = get_model()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> xb, yb <span class="keyword">in</span> train_dl:</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line">        </span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line">        </span><br><span class="line">    model.eval()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        valid_loss = sum(loss_func(model(xb), yb) <span class="keyword">for</span> xb, yb <span class="keyword">in</span> valid_dl)</span><br><span class="line"></span><br><span class="line">    print(epoch, valid_loss / len(valid_dl))</span><br></pre></td></tr></table></figure>

<pre><code>0 tensor(0.3312)
1 tensor(0.4040)</code></pre><h2 id="Create-fit-and-get-data"><a href="#Create-fit-and-get-data" class="headerlink" title="Create fit() and get_data()"></a>Create fit() and get_data()</h2><p>æ¥ä¸‹æ¥æˆ‘ä»¬è‡ªå·±å¯¹ä»£ç è¿›è¡Œä¸€äº›é‡æ„ã€‚æ³¨æ„åˆ°æˆ‘ä»¬åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸Šéƒ½è¦è®¡ç®— lossï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥æŠŠè¿™ä¸ªè¿‡ç¨‹æŠ½è±¡æˆä¸€ä¸ªå‡½æ•° <code>loss_batch</code>.</p>
<p>å¦å¤–ï¼Œåœ¨è®­ç»ƒé›†ä¸Šè®¡ç®— loss æ—¶ï¼Œæˆ‘ä»¬ä¼ å…¥ä¸€ä¸ª optimizer æ¥æ‰§è¡Œåå‘ä¼ æ’­ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_batch</span><span class="params">(model, loss_func, xb, yb, opt=None)</span>:</span></span><br><span class="line">    loss = loss_func(model(xb), yb)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss.item(), len(xb)</span><br></pre></td></tr></table></figure>

<p>å†æ„é€ ä¸€ä¸ª <code>fit</code> å‡½æ•°æ¥æ‰§è¡Œè®­ç»ƒé˜¶æ®µçš„å¿…è¦æ“ä½œï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(epochs, model, loss_func, opt, train_dl, valid_dl)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">        model.train()</span><br><span class="line">        <span class="keyword">for</span> xb, yb <span class="keyword">in</span> train_dl:</span><br><span class="line">            loss_batch(model, loss_func, xb, yb, opt)</span><br><span class="line">            </span><br><span class="line">        model.eval()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            losses, nums = zip(</span><br><span class="line">                *[loss_batch(model, loss_func, xb, yb) <span class="keyword">for</span> xb, yb <span class="keyword">in</span> valid_dl]</span><br><span class="line">            )</span><br><span class="line">        valid_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)</span><br><span class="line">        </span><br><span class="line">        print(epoch, valid_loss)</span><br></pre></td></tr></table></figure>

<p><code>get_data</code> è¿”å›è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„ dataloaderï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">(train_ds, valid_ds, bs)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        DataLoader(train_ds, batch_size=bs, shuffle=<span class="literal">True</span>),</span><br><span class="line">        DataLoader(valid_ds, batch_size=bs * <span class="number">2</span>)</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>

<p>è‡³æ­¤ï¼Œæˆ‘ä»¬çš„æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹å¯ä»¥ç”¨ä¸‰è¡Œä»£ç è§£å†³ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_dl, valid_dl = get_data(train_ds, valid_ds, bs)</span><br><span class="line">model, opt = get_model()</span><br><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>

<pre><code>0 0.3082067723274231
1 0.4469423895835877</code></pre><h2 id="Switch-to-CNN"><a href="#Switch-to-CNN" class="headerlink" title="Switch to CNN"></a>Switch to CNN</h2><p>æ¥ä¸‹æ¥æˆ‘ä»¬ä½¿ç”¨ä¸‰ä¸ªå·ç§¯å±‚æ¥æ„å»ºç¥ç»ç½‘ç»œã€‚ç”±äºä¹‹å‰å®šä¹‰çš„è¾…åŠ©å‡½æ•°ä¸æ¨¡å‹æ˜¯æ— å…³çš„ï¼Œå› æ­¤æˆ‘ä»¬ä»ç„¶å¯ä»¥ä½¿ç”¨å®ƒä»¬æ¥è®­ç»ƒ CNN.</p>
<p>æˆ‘ä»¬å°†ä½¿ç”¨ PyTorch çš„ <code>Conv2d</code> ç±»ä½œä¸ºå·ç§¯å±‚ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸‰å±‚å·ç§¯çš„ç½‘ç»œï¼Œæ¯å±‚å·ç§¯çš„ç»“æœéƒ½ç”¨ ReLU ä½œä¸ºæ¿€åŠ±å‡½æ•°ï¼Œæœ€åæˆ‘ä»¬ä¼šæ‰§è¡Œä¸€ä¸ªå¹³å‡æ± åŒ–ï¼ˆaverage poolingï¼‰ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mnist_CNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        <span class="comment"># input: 28x28</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>) <span class="comment"># 14x14</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>) <span class="comment"># 7x7</span></span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">16</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">2</span>) <span class="comment"># 5x5</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, xb)</span>:</span></span><br><span class="line">        xb = xb.view(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">        xb = F.relu(self.conv1(xb))</span><br><span class="line">        xb = F.relu(self.conv2(xb))</span><br><span class="line">        xb = F.relu(self.conv3(xb))</span><br><span class="line">        xb = F.avg_pool2d(xb, <span class="number">4</span>) <span class="comment"># 1x1</span></span><br><span class="line">        <span class="keyword">return</span> xb.view(<span class="number">-1</span>, xb.size(<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">lr = <span class="number">0.1</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = Mnist_CNN()</span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>

<pre><code>0 0.3150287981033325
1 0.23092529454231261</code></pre><p><a href="https://cs231n.github.io/neural-networks-3/#sgd" target="_blank" rel="noopener">Momentum</a> æ˜¯éšæœºæ¢¯åº¦ä¸‹é™çš„ä¸€ä¸ªå˜ä½“ï¼Œå®ƒåœ¨æ‰§è¡Œæƒé‡æ›´æ–°çš„æ—¶å€™ä¹Ÿä¼šè€ƒè™‘ä¹‹å‰çš„æ›´æ–°æ“ä½œï¼Œä¸€èˆ¬æƒ…å†µä¸‹ä¼šä½¿å¾—è®­ç»ƒé€Ÿåº¦åŠ å¿«ã€‚</p>
<h2 id="nn-Sequential"><a href="#nn-Sequential" class="headerlink" title="nn.Sequential"></a>nn.Sequential</h2><p><code>Sequential</code> å¯¹è±¡ä¼šæŒ‰é¡ºåºæ‰§è¡Œå…¶å†…éƒ¨åŒ…å«çš„æ¨¡å—ï¼Œè¿™æ˜¯æ„å»ºç¥ç»ç½‘ç»œçš„ä¸€ç§ç®€å•çš„åšæ³•ã€‚</p>
<p>æˆ‘ä»¬å¯ä»¥åœ¨ <code>Sequential</code> å¯¹è±¡ä¸­æ”¾å…¥è‡ªå®šä¹‰çš„å±‚ï¼Œä¾‹å¦‚è¯´ï¼ŒPyTorch åŸç”Ÿæ²¡æœ‰ view layerï¼Œæˆ‘ä»¬æ„é€ ä¸€ä¸ª <code>Lambda</code> å‡½æ•°æ¥å¯¹ <code>Sequential</code> ä¼ å…¥è‡ªå®šä¹‰çš„å±‚ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Lambda</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, func)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.func = func</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.func(x)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x.view(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">    Lambda(preprocess),</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.AvgPool2d(<span class="number">4</span>),</span><br><span class="line">    Lambda(<span class="keyword">lambda</span> x: x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>))</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>

<pre><code>0 0.3572545750617981
1 0.2796625346660614</code></pre><h2 id="Wrapping-DataLoader"><a href="#Wrapping-DataLoader" class="headerlink" title="Wrapping DataLoader"></a>Wrapping DataLoader</h2><p>ä¸Šè¿°çš„ CNN æ¨¡å‹å¾ˆç®€æ´ï¼Œä½†æ˜¯åªèƒ½ç”¨äº MNISTï¼ŒåŸå› æ˜¯ï¼š</p>
<ul>
<li>æ¨¡å‹å‡å®šè¾“å…¥æ•°æ®æ˜¯ 28x28 çš„å‘é‡</li>
<li>æ¨¡å‹å‡å®š CNN æœ€åä¸€å±‚çš„ç½‘æ ¼å¤§å°æ˜¯ 4x4ï¼ˆå› ä¸ºæˆ‘ä»¬ä½¿ç”¨äº† 4x4 çš„å¹³å‡æ± åŒ–ï¼‰</li>
</ul>
<p>ä¸‹é¢è®©æˆ‘ä»¬æƒ³åŠæ³•è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥æŠŠæ•°æ®é¢„å¤„ç†ä» Lambda å±‚ä¸­å»é™¤ï¼ŒæŠŠå®ƒæ”¾åˆ°ä¸€ä¸ªç”Ÿæˆå™¨ä¸­ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x.view(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WrappedDataLoader</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dl, func)</span>:</span></span><br><span class="line">        self.dl = dl</span><br><span class="line">        self.func = func</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.dl)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        batches = iter(self.dl)</span><br><span class="line">        <span class="keyword">for</span> b <span class="keyword">in</span> batches:</span><br><span class="line">            <span class="keyword">yield</span> (self.func(*b))</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">train_dl, valid_dl = get_data(train_ds, valid_ds, bs)</span><br><span class="line">train_dl = WrappedDataLoader(train_dl, preprocess)</span><br><span class="line">valid_dl = WrappedDataLoader(valid_dl, preprocess)</span><br></pre></td></tr></table></figure>

<p>æ¥ä¸‹æ¥æˆ‘ä»¬ä½¿ç”¨ <code>nn.AdaptiveAvgPool2d</code> æ¥æ›¿æ¢ <code>nn.AvgPool2d</code>, è¿™ä½¿å¾—æˆ‘ä»¬å¯ä»¥å®šä¹‰è¾“å‡ºå¼ é‡çš„å¤§å°è€Œä¸æ˜¯è¾“å…¥å¼ é‡çš„å¤§å°ã€‚è¿™æ ·åšçš„ç»“æœæ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥æ¥å—ä»»æ„å¤§å°çš„è¾“å…¥ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.AdaptiveAvgPool2d(<span class="number">1</span>), <span class="comment"># è¾“å‡ºå¤§å°ä¸º 1x1</span></span><br><span class="line">    Lambda(<span class="keyword">lambda</span> x: x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>

<pre><code>0 0.31847067918777466
1 0.24730257654190063</code></pre><h2 id="Using-your-GPU"><a href="#Using-your-GPU" class="headerlink" title="Using your GPU"></a>Using your GPU</h2><p>ç•¥ã€‚</p>
<hr>
<p>å‚è€ƒèµ„æ–™ï¼š</p>
<ul>
<li><a href="https://pytorch.org/tutorials/beginner/nn_tutorial.html" target="_blank" rel="noopener">WHAT IS TORCH.NN REALLY?</a></li>
</ul>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        
          
        
        <div class="post-tags">
          
            <a href="/tags/pytorch/" rel="tag"># pytorch</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/06/25/pytorch-saving-and-loading-models/" rel="next" title="Saving and Loading Models">
                <i class="fa fa-chevron-left"></i> Saving and Loading Models
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/09/19/vscode-debug-via-cmake-in-ubuntu/" rel="prev" title="Vscode Debug via Cmake in Ubuntu">
                Vscode Debug via Cmake in Ubuntu <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            æ–‡ç« ç›®å½•
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            ç«™ç‚¹æ¦‚è§ˆ
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Patrick</p>
              <div class="site-description motion-element" itemprop="description">ğŸŒˆ</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">16</span>
                    <span class="site-state-item-name">æ—¥å¿—</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">5</span>
                    <span class="site-state-item-name">æ ‡ç­¾</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#MNIST-data-setup"><span class="nav-number">1.</span> <span class="nav-text">MNIST data setup</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Neural-net-from-scratch-no-torch-nn"><span class="nav-number">2.</span> <span class="nav-text">Neural net from scratch(no torch.nn)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Using-torch-nn-functional"><span class="nav-number">3.</span> <span class="nav-text">Using torch.nn.functional</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Refactor-using-nn-Module"><span class="nav-number">4.</span> <span class="nav-text">Refactor using nn.Module</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Refactor-using-nn-Linear"><span class="nav-number">5.</span> <span class="nav-text">Refactor using nn.Linear</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Refactor-using-optim"><span class="nav-number">6.</span> <span class="nav-text">Refactor using optim</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Refactor-using-Dataset"><span class="nav-number">7.</span> <span class="nav-text">Refactor using Dataset</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Refactor-using-DataLoader"><span class="nav-number">8.</span> <span class="nav-text">Refactor using DataLoader</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Add-Validation"><span class="nav-number">9.</span> <span class="nav-text">Add Validation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Create-fit-and-get-data"><span class="nav-number">10.</span> <span class="nav-text">Create fit() and get_data()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Switch-to-CNN"><span class="nav-number">11.</span> <span class="nav-text">Switch to CNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#nn-Sequential"><span class="nav-number">12.</span> <span class="nav-text">nn.Sequential</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Wrapping-DataLoader"><span class="nav-number">13.</span> <span class="nav-text">Wrapping DataLoader</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Using-your-GPU"><span class="nav-number">14.</span> <span class="nav-text">Using your GPU</span></a></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Patrick</span>

  

  
</div>









        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.1.2"></script>

  <script src="/js/motion.js?v=7.1.2"></script>



  
  


  <script src="/js/affix.js?v=7.1.2"></script>

  <script src="/js/schemes/pisces.js?v=7.1.2"></script>



  
  <script src="/js/scrollspy.js?v=7.1.2"></script>
<script src="/js/post-details.js?v=7.1.2"></script>



  


  <script src="/js/next-boot.js?v=7.1.2"></script>


  

  

  

  


  


  




  

  

  
  

  
  

  


  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
